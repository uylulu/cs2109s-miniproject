{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcb2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77dc61",
   "metadata": {},
   "source": [
    "### Load the dataset and pre-process them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21214c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"data/cipher_objective.csv\")\n",
    "df_x = data_set[\"text\"]\n",
    "df_y = data_set[\"class\"]\n",
    "\n",
    "arr_x = []\n",
    "arr_y = []\n",
    "\n",
    "for i in range(len(df_x)):\n",
    "    text = df_x.iloc[i]\n",
    "    temp = [ord(c) for c in text]\n",
    "    arr_x.append(temp)\n",
    "\n",
    "    label = df_y.iloc[i]\n",
    "    arr_y.append(0 if label == \"exit\" else 1)\n",
    "\n",
    "data_x = torch.tensor(arr_x, dtype=torch.float32)\n",
    "data_y = torch.tensor(arr_y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2d8fe",
   "metadata": {},
   "source": [
    "### Split the dataset and load them into data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51c1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 100]) torch.Size([2400]) torch.Size([1600, 100]) torch.Size([1600])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  data_x, data_y,\n",
    "  test_size=0.4,\n",
    "  stratify=data_y,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "train_x = X_train.detach().clone()\n",
    "train_y = y_train.detach().clone()\n",
    "test_x = X_test.detach().clone()\n",
    "test_y = y_test.detach().clone()\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0c8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1600, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a460850",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb6f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size: int):\n",
    "    super().__init__()\n",
    "    self.l1 = nn.Linear(input_size, 1, True)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.l1(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2d7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(loader: torch.utils.data.DataLoader, model: nn.Module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model.to(device)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "    epoch_losses = []\n",
    "    for i in range(300):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for _, (x, y) in enumerate(loader):\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            y = y.view(-1, 1).long()   \n",
    "            loss = loss_fn(y_pred, y.float())\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(loader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "        \n",
    "\n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d025e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 74.3392988204956\n",
      "Epoch: 1, Loss: 83.12007522583008\n",
      "Epoch: 2, Loss: 80.28693923950195\n",
      "Epoch: 3, Loss: 75.65507049560547\n",
      "Epoch: 4, Loss: 76.28227291107177\n",
      "Epoch: 5, Loss: 72.65942840576172\n",
      "Epoch: 6, Loss: 71.7201473236084\n",
      "Epoch: 7, Loss: 68.56741142272949\n",
      "Epoch: 8, Loss: 67.76617736816407\n",
      "Epoch: 9, Loss: 64.4251594543457\n",
      "Epoch: 10, Loss: 69.57161254882813\n",
      "Epoch: 11, Loss: 61.711208724975585\n",
      "Epoch: 12, Loss: 63.781436157226565\n",
      "Epoch: 13, Loss: 62.946955871582034\n",
      "Epoch: 14, Loss: 61.13170280456543\n",
      "Epoch: 15, Loss: 55.6065975189209\n",
      "Epoch: 16, Loss: 55.38992862701416\n",
      "Epoch: 17, Loss: 55.69268112182617\n",
      "Epoch: 18, Loss: 52.65674819946289\n",
      "Epoch: 19, Loss: 57.7535587310791\n",
      "Epoch: 20, Loss: 51.32097473144531\n",
      "Epoch: 21, Loss: 53.903232383728025\n",
      "Epoch: 22, Loss: 47.677008628845215\n",
      "Epoch: 23, Loss: 48.27898254394531\n",
      "Epoch: 24, Loss: 49.23761520385742\n",
      "Epoch: 25, Loss: 42.92498207092285\n",
      "Epoch: 26, Loss: 49.693812942504884\n",
      "Epoch: 27, Loss: 44.92315158843994\n",
      "Epoch: 28, Loss: 41.916252326965335\n",
      "Epoch: 29, Loss: 42.45442714691162\n",
      "Epoch: 30, Loss: 37.27211303710938\n",
      "Epoch: 31, Loss: 39.91522130966187\n",
      "Epoch: 32, Loss: 33.2546667098999\n",
      "Epoch: 33, Loss: 28.999010467529295\n",
      "Epoch: 34, Loss: 27.835384941101076\n",
      "Epoch: 35, Loss: 27.748454093933105\n",
      "Epoch: 36, Loss: 23.618341636657714\n",
      "Epoch: 37, Loss: 25.359708499908447\n",
      "Epoch: 38, Loss: 29.539030838012696\n",
      "Epoch: 39, Loss: 15.398544216156006\n",
      "Epoch: 40, Loss: 6.696302652359009\n",
      "Epoch: 41, Loss: 6.30007266998291\n",
      "Epoch: 42, Loss: 19.75660228729248\n",
      "Epoch: 43, Loss: 25.804469871520997\n",
      "Epoch: 44, Loss: 25.948265075683594\n",
      "Epoch: 45, Loss: 13.097584533691407\n",
      "Epoch: 46, Loss: 16.149577808380126\n",
      "Epoch: 47, Loss: 13.765576505661011\n",
      "Epoch: 48, Loss: 6.407117033004761\n",
      "Epoch: 49, Loss: 6.903622245788574\n",
      "Epoch: 50, Loss: 15.393473720550537\n",
      "Epoch: 51, Loss: 19.88765983581543\n",
      "Epoch: 52, Loss: 14.628058242797852\n",
      "Epoch: 53, Loss: 12.326344871520996\n",
      "Epoch: 54, Loss: 12.395252132415772\n",
      "Epoch: 55, Loss: 1.7338488340377807\n",
      "Epoch: 56, Loss: 2.353665220737457\n",
      "Epoch: 57, Loss: 1.6276964664459228\n",
      "Epoch: 58, Loss: 3.05449640750885\n",
      "Epoch: 59, Loss: 6.945655596256256\n",
      "Epoch: 60, Loss: 5.758992302417755\n",
      "Epoch: 61, Loss: 1.5716303825378417\n",
      "Epoch: 62, Loss: 1.7232485294342041\n",
      "Epoch: 63, Loss: 1.1964206755161286\n",
      "Epoch: 64, Loss: 1.323352086544037\n",
      "Epoch: 65, Loss: 3.385320234298706\n",
      "Epoch: 66, Loss: 2.1569994285702707\n",
      "Epoch: 67, Loss: 0.9958483189344406\n",
      "Epoch: 68, Loss: 0.9551052480936051\n",
      "Epoch: 69, Loss: 3.1058571100234986\n",
      "Epoch: 70, Loss: 1.3692518293857574\n",
      "Epoch: 71, Loss: 0.9820585370063781\n",
      "Epoch: 72, Loss: 1.6037189781665802\n",
      "Epoch: 73, Loss: 1.2196970939636231\n",
      "Epoch: 74, Loss: 0.8129624456167222\n",
      "Epoch: 75, Loss: 1.2799210131168366\n",
      "Epoch: 76, Loss: 1.034528548270464\n",
      "Epoch: 77, Loss: 1.0452631384134292\n",
      "Epoch: 78, Loss: 1.10714653134346\n",
      "Epoch: 79, Loss: 0.7528871729969978\n",
      "Epoch: 80, Loss: 2.186909744143486\n",
      "Epoch: 81, Loss: 5.726222586631775\n",
      "Epoch: 82, Loss: 0.751933366060257\n",
      "Epoch: 83, Loss: 0.9798449575901031\n",
      "Epoch: 84, Loss: 0.825554521381855\n",
      "Epoch: 85, Loss: 0.7207525074481964\n",
      "Epoch: 86, Loss: 1.0036815732717514\n",
      "Epoch: 87, Loss: 0.8588444143533707\n",
      "Epoch: 88, Loss: 0.7982909500598907\n",
      "Epoch: 89, Loss: 0.7344695072621107\n",
      "Epoch: 90, Loss: 5.748382556438446\n",
      "Epoch: 91, Loss: 0.8045898407697678\n",
      "Epoch: 92, Loss: 0.6896513760089874\n",
      "Epoch: 93, Loss: 1.0902093946933746\n",
      "Epoch: 94, Loss: 2.5026185393333433\n",
      "Epoch: 95, Loss: 1.335650385171175\n",
      "Epoch: 96, Loss: 1.3821543335914612\n",
      "Epoch: 97, Loss: 2.90454523563385\n",
      "Epoch: 98, Loss: 3.3773588120937346\n",
      "Epoch: 99, Loss: 9.228130841255188\n",
      "Epoch: 100, Loss: 11.150836420059203\n",
      "Epoch: 101, Loss: 7.296803903579712\n",
      "Epoch: 102, Loss: 13.65786690711975\n",
      "Epoch: 103, Loss: 17.680661106109618\n",
      "Epoch: 104, Loss: 3.8450915455818175\n",
      "Epoch: 105, Loss: 2.1553982496261597\n",
      "Epoch: 106, Loss: 2.5913076400756836\n",
      "Epoch: 107, Loss: 1.560740377753973\n",
      "Epoch: 108, Loss: 0.6663596883416176\n",
      "Epoch: 109, Loss: 0.7393460618332028\n",
      "Epoch: 110, Loss: 0.6591475307941437\n",
      "Epoch: 111, Loss: 0.6662395246326923\n",
      "Epoch: 112, Loss: 0.7004324585199356\n",
      "Epoch: 113, Loss: 2.779148644208908\n",
      "Epoch: 114, Loss: 1.2330840572714805\n",
      "Epoch: 115, Loss: 0.6805716998875141\n",
      "Epoch: 116, Loss: 0.6876947335898876\n",
      "Epoch: 117, Loss: 0.6245462030172348\n",
      "Epoch: 118, Loss: 1.3822015911340713\n",
      "Epoch: 119, Loss: 0.6459549933671951\n",
      "Epoch: 120, Loss: 0.8688431303948164\n",
      "Epoch: 121, Loss: 0.7260572348721326\n",
      "Epoch: 122, Loss: 0.5911882951855659\n",
      "Epoch: 123, Loss: 1.1923588670790195\n",
      "Epoch: 124, Loss: 0.6921961478888988\n",
      "Epoch: 125, Loss: 3.5479074239730837\n",
      "Epoch: 126, Loss: 10.180988264083862\n",
      "Epoch: 127, Loss: 7.168428564071656\n",
      "Epoch: 128, Loss: 3.197691261768341\n",
      "Epoch: 129, Loss: 0.8162495866417885\n",
      "Epoch: 130, Loss: 0.740990637242794\n",
      "Epoch: 131, Loss: 0.6373348362743855\n",
      "Epoch: 132, Loss: 0.6227591834962368\n",
      "Epoch: 133, Loss: 0.6942213274538517\n",
      "Epoch: 134, Loss: 0.7193172764033079\n",
      "Epoch: 135, Loss: 2.550731545686722\n",
      "Epoch: 136, Loss: 2.985149449110031\n",
      "Epoch: 137, Loss: 0.5794439680874348\n",
      "Epoch: 138, Loss: 0.6563787780702114\n",
      "Epoch: 139, Loss: 0.6737937644124031\n",
      "Epoch: 140, Loss: 0.5672128837555647\n",
      "Epoch: 141, Loss: 0.870005989074707\n",
      "Epoch: 142, Loss: 0.5272054748144\n",
      "Epoch: 143, Loss: 0.554440176486969\n",
      "Epoch: 144, Loss: 0.6500457551330328\n",
      "Epoch: 145, Loss: 3.771521282196045\n",
      "Epoch: 146, Loss: 2.650035160779953\n",
      "Epoch: 147, Loss: 0.5729787111282348\n",
      "Epoch: 148, Loss: 0.9393204525113106\n",
      "Epoch: 149, Loss: 3.608709394931793\n",
      "Epoch: 150, Loss: 0.9008684694766999\n",
      "Epoch: 151, Loss: 0.7069461036473512\n",
      "Epoch: 152, Loss: 0.5802313216496259\n",
      "Epoch: 153, Loss: 0.6450423441827298\n",
      "Epoch: 154, Loss: 1.244331419467926\n",
      "Epoch: 155, Loss: 0.553926817048341\n",
      "Epoch: 156, Loss: 0.5498048983514309\n",
      "Epoch: 157, Loss: 0.5492208682000637\n",
      "Epoch: 158, Loss: 0.5832105251960457\n",
      "Epoch: 159, Loss: 0.5603693928569555\n",
      "Epoch: 160, Loss: 0.5337116800248622\n",
      "Epoch: 161, Loss: 0.6889535034541041\n",
      "Epoch: 162, Loss: 0.6011752238497138\n",
      "Epoch: 163, Loss: 0.6862565964460373\n",
      "Epoch: 164, Loss: 0.5531947296112776\n",
      "Epoch: 165, Loss: 0.5427584435790778\n",
      "Epoch: 166, Loss: 0.5503311462700367\n",
      "Epoch: 167, Loss: 0.5517601509130146\n",
      "Epoch: 168, Loss: 0.5367279082536698\n",
      "Epoch: 169, Loss: 2.54752054810524\n",
      "Epoch: 170, Loss: 4.906094002723694\n",
      "Epoch: 171, Loss: 9.92885103225708\n",
      "Epoch: 172, Loss: 3.4750600337982176\n",
      "Epoch: 173, Loss: 0.7413829120807349\n",
      "Epoch: 174, Loss: 2.7921658158302307\n",
      "Epoch: 175, Loss: 0.6531695540994406\n",
      "Epoch: 176, Loss: 0.5759968988597393\n",
      "Epoch: 177, Loss: 0.8828467488288879\n",
      "Epoch: 178, Loss: 0.5456912063062191\n",
      "Epoch: 179, Loss: 0.9159355618059635\n",
      "Epoch: 180, Loss: 0.522395984083414\n",
      "Epoch: 181, Loss: 0.9075418528635055\n",
      "Epoch: 182, Loss: 4.442626953125\n",
      "Epoch: 183, Loss: 7.767560821771622\n",
      "Epoch: 184, Loss: 0.5967112246900796\n",
      "Epoch: 185, Loss: 0.5542926535010337\n",
      "Epoch: 186, Loss: 0.7225287243723869\n",
      "Epoch: 187, Loss: 0.8979916259646415\n",
      "Epoch: 188, Loss: 0.5630660690367222\n",
      "Epoch: 189, Loss: 0.7376041576266289\n",
      "Epoch: 190, Loss: 0.5829350061714649\n",
      "Epoch: 191, Loss: 1.6777095317840576\n",
      "Epoch: 192, Loss: 0.511769592948258\n",
      "Epoch: 193, Loss: 0.564829969778657\n",
      "Epoch: 194, Loss: 0.569826252758503\n",
      "Epoch: 195, Loss: 0.5604617065284401\n",
      "Epoch: 196, Loss: 0.591907930932939\n",
      "Epoch: 197, Loss: 0.5564604382961988\n",
      "Epoch: 198, Loss: 0.5496116623282432\n",
      "Epoch: 199, Loss: 0.6579113990068436\n",
      "Epoch: 200, Loss: 4.467308592796326\n",
      "Epoch: 201, Loss: 2.090949738025665\n",
      "Epoch: 202, Loss: 3.3116106897592545\n",
      "Epoch: 203, Loss: 0.7542257843539119\n",
      "Epoch: 204, Loss: 0.5523518776580204\n",
      "Epoch: 205, Loss: 0.6658123522996903\n",
      "Epoch: 206, Loss: 0.7049356706440448\n",
      "Epoch: 207, Loss: 0.5282835721969604\n",
      "Epoch: 208, Loss: 0.5603218057192862\n",
      "Epoch: 209, Loss: 1.4116967871785164\n",
      "Epoch: 210, Loss: 1.2964799858629703\n",
      "Epoch: 211, Loss: 0.5454071320593357\n",
      "Epoch: 212, Loss: 3.9601753652095795\n",
      "Epoch: 213, Loss: 7.390788006782532\n",
      "Epoch: 214, Loss: 2.408160275220871\n",
      "Epoch: 215, Loss: 1.431661108136177\n",
      "Epoch: 216, Loss: 2.004127186536789\n",
      "Epoch: 217, Loss: 3.0094690918922424\n",
      "Epoch: 218, Loss: 13.75387077331543\n",
      "Epoch: 219, Loss: 6.068401467800141\n",
      "Epoch: 220, Loss: 0.5322012424934656\n",
      "Epoch: 221, Loss: 0.690264733787626\n",
      "Epoch: 222, Loss: 0.5385294053703547\n",
      "Epoch: 223, Loss: 0.7431977070868016\n",
      "Epoch: 224, Loss: 0.5808646656572819\n",
      "Epoch: 225, Loss: 0.6256781980395317\n",
      "Epoch: 226, Loss: 0.5519132561981678\n",
      "Epoch: 227, Loss: 0.6272472651675344\n",
      "Epoch: 228, Loss: 0.5197511862963438\n",
      "Epoch: 229, Loss: 0.5266077972948551\n",
      "Epoch: 230, Loss: 0.554773635417223\n",
      "Epoch: 231, Loss: 0.520250310190022\n",
      "Epoch: 232, Loss: 0.7371049106121064\n",
      "Epoch: 233, Loss: 0.8806909292936325\n",
      "Epoch: 234, Loss: 2.501278005389031\n",
      "Epoch: 235, Loss: 1.3685579359531403\n",
      "Epoch: 236, Loss: 0.8410668969154358\n",
      "Epoch: 237, Loss: 0.6059177195653319\n",
      "Epoch: 238, Loss: 0.5927757825702429\n",
      "Epoch: 239, Loss: 0.512636942602694\n",
      "Epoch: 240, Loss: 0.589342413470149\n",
      "Epoch: 241, Loss: 0.66898078545928\n",
      "Epoch: 242, Loss: 0.7291197683662176\n",
      "Epoch: 243, Loss: 0.6591323539614677\n",
      "Epoch: 244, Loss: 0.5329935912042856\n",
      "Epoch: 245, Loss: 1.2613463554531337\n",
      "Epoch: 246, Loss: 1.043851485848427\n",
      "Epoch: 247, Loss: 1.4161022753454744\n",
      "Epoch: 248, Loss: 0.5204496828839182\n",
      "Epoch: 249, Loss: 0.5305062349885702\n",
      "Epoch: 250, Loss: 0.6793022289872169\n",
      "Epoch: 251, Loss: 0.5107326129684224\n",
      "Epoch: 252, Loss: 0.5137807443737984\n",
      "Epoch: 253, Loss: 0.787365884706378\n",
      "Epoch: 254, Loss: 0.49302118085324764\n",
      "Epoch: 255, Loss: 0.6372616270091385\n",
      "Epoch: 256, Loss: 4.250388273596764\n",
      "Epoch: 257, Loss: 2.2035419762134554\n",
      "Epoch: 258, Loss: 2.336372983455658\n",
      "Epoch: 259, Loss: 1.6412193089723588\n",
      "Epoch: 260, Loss: 0.5731876626610756\n",
      "Epoch: 261, Loss: 0.7149516008794308\n",
      "Epoch: 262, Loss: 0.7398112624883652\n",
      "Epoch: 263, Loss: 0.7252553605940193\n",
      "Epoch: 264, Loss: 0.5034627355635166\n",
      "Epoch: 265, Loss: 0.5415327908471227\n",
      "Epoch: 266, Loss: 0.5078989751636982\n",
      "Epoch: 267, Loss: 0.8379516210407019\n",
      "Epoch: 268, Loss: 5.020260202884674\n",
      "Epoch: 269, Loss: 1.9189450562000274\n",
      "Epoch: 270, Loss: 1.9045753836631776\n",
      "Epoch: 271, Loss: 0.5105962681409437\n",
      "Epoch: 272, Loss: 0.5283342435956001\n",
      "Epoch: 273, Loss: 0.7942133557051421\n",
      "Epoch: 274, Loss: 0.7708092212677002\n",
      "Epoch: 275, Loss: 0.5945615917444229\n",
      "Epoch: 276, Loss: 0.5782835766673088\n",
      "Epoch: 277, Loss: 0.49511607238091526\n",
      "Epoch: 278, Loss: 0.5441994376480579\n",
      "Epoch: 279, Loss: 0.5856028160080313\n",
      "Epoch: 280, Loss: 0.7321796596050263\n",
      "Epoch: 281, Loss: 0.6144633609801531\n",
      "Epoch: 282, Loss: 0.6015920888632535\n",
      "Epoch: 283, Loss: 0.6265232473611831\n",
      "Epoch: 284, Loss: 1.8633198738098145\n",
      "Epoch: 285, Loss: 2.2499433398246764\n",
      "Epoch: 286, Loss: 0.5274443790316582\n",
      "Epoch: 287, Loss: 0.4890538577221378\n",
      "Epoch: 288, Loss: 0.549083049967885\n",
      "Epoch: 289, Loss: 0.7374102577567101\n",
      "Epoch: 290, Loss: 0.7331961406394839\n",
      "Epoch: 291, Loss: 0.6577908150851727\n",
      "Epoch: 292, Loss: 0.5119014862924814\n",
      "Epoch: 293, Loss: 0.5633708868874236\n",
      "Epoch: 294, Loss: 0.5538820177316666\n",
      "Epoch: 295, Loss: 1.045443357527256\n",
      "Epoch: 296, Loss: 0.9338945746421814\n",
      "Epoch: 297, Loss: 0.9194879680871964\n",
      "Epoch: 298, Loss: 0.5765351135283708\n",
      "Epoch: 299, Loss: 0.5383061174303293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (l1): Linear(in_features=100, out_features=1, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " [74.3392988204956,\n",
       "  83.12007522583008,\n",
       "  80.28693923950195,\n",
       "  75.65507049560547,\n",
       "  76.28227291107177,\n",
       "  72.65942840576172,\n",
       "  71.7201473236084,\n",
       "  68.56741142272949,\n",
       "  67.76617736816407,\n",
       "  64.4251594543457,\n",
       "  69.57161254882813,\n",
       "  61.711208724975585,\n",
       "  63.781436157226565,\n",
       "  62.946955871582034,\n",
       "  61.13170280456543,\n",
       "  55.6065975189209,\n",
       "  55.38992862701416,\n",
       "  55.69268112182617,\n",
       "  52.65674819946289,\n",
       "  57.7535587310791,\n",
       "  51.32097473144531,\n",
       "  53.903232383728025,\n",
       "  47.677008628845215,\n",
       "  48.27898254394531,\n",
       "  49.23761520385742,\n",
       "  42.92498207092285,\n",
       "  49.693812942504884,\n",
       "  44.92315158843994,\n",
       "  41.916252326965335,\n",
       "  42.45442714691162,\n",
       "  37.27211303710938,\n",
       "  39.91522130966187,\n",
       "  33.2546667098999,\n",
       "  28.999010467529295,\n",
       "  27.835384941101076,\n",
       "  27.748454093933105,\n",
       "  23.618341636657714,\n",
       "  25.359708499908447,\n",
       "  29.539030838012696,\n",
       "  15.398544216156006,\n",
       "  6.696302652359009,\n",
       "  6.30007266998291,\n",
       "  19.75660228729248,\n",
       "  25.804469871520997,\n",
       "  25.948265075683594,\n",
       "  13.097584533691407,\n",
       "  16.149577808380126,\n",
       "  13.765576505661011,\n",
       "  6.407117033004761,\n",
       "  6.903622245788574,\n",
       "  15.393473720550537,\n",
       "  19.88765983581543,\n",
       "  14.628058242797852,\n",
       "  12.326344871520996,\n",
       "  12.395252132415772,\n",
       "  1.7338488340377807,\n",
       "  2.353665220737457,\n",
       "  1.6276964664459228,\n",
       "  3.05449640750885,\n",
       "  6.945655596256256,\n",
       "  5.758992302417755,\n",
       "  1.5716303825378417,\n",
       "  1.7232485294342041,\n",
       "  1.1964206755161286,\n",
       "  1.323352086544037,\n",
       "  3.385320234298706,\n",
       "  2.1569994285702707,\n",
       "  0.9958483189344406,\n",
       "  0.9551052480936051,\n",
       "  3.1058571100234986,\n",
       "  1.3692518293857574,\n",
       "  0.9820585370063781,\n",
       "  1.6037189781665802,\n",
       "  1.2196970939636231,\n",
       "  0.8129624456167222,\n",
       "  1.2799210131168366,\n",
       "  1.034528548270464,\n",
       "  1.0452631384134292,\n",
       "  1.10714653134346,\n",
       "  0.7528871729969978,\n",
       "  2.186909744143486,\n",
       "  5.726222586631775,\n",
       "  0.751933366060257,\n",
       "  0.9798449575901031,\n",
       "  0.825554521381855,\n",
       "  0.7207525074481964,\n",
       "  1.0036815732717514,\n",
       "  0.8588444143533707,\n",
       "  0.7982909500598907,\n",
       "  0.7344695072621107,\n",
       "  5.748382556438446,\n",
       "  0.8045898407697678,\n",
       "  0.6896513760089874,\n",
       "  1.0902093946933746,\n",
       "  2.5026185393333433,\n",
       "  1.335650385171175,\n",
       "  1.3821543335914612,\n",
       "  2.90454523563385,\n",
       "  3.3773588120937346,\n",
       "  9.228130841255188,\n",
       "  11.150836420059203,\n",
       "  7.296803903579712,\n",
       "  13.65786690711975,\n",
       "  17.680661106109618,\n",
       "  3.8450915455818175,\n",
       "  2.1553982496261597,\n",
       "  2.5913076400756836,\n",
       "  1.560740377753973,\n",
       "  0.6663596883416176,\n",
       "  0.7393460618332028,\n",
       "  0.6591475307941437,\n",
       "  0.6662395246326923,\n",
       "  0.7004324585199356,\n",
       "  2.779148644208908,\n",
       "  1.2330840572714805,\n",
       "  0.6805716998875141,\n",
       "  0.6876947335898876,\n",
       "  0.6245462030172348,\n",
       "  1.3822015911340713,\n",
       "  0.6459549933671951,\n",
       "  0.8688431303948164,\n",
       "  0.7260572348721326,\n",
       "  0.5911882951855659,\n",
       "  1.1923588670790195,\n",
       "  0.6921961478888988,\n",
       "  3.5479074239730837,\n",
       "  10.180988264083862,\n",
       "  7.168428564071656,\n",
       "  3.197691261768341,\n",
       "  0.8162495866417885,\n",
       "  0.740990637242794,\n",
       "  0.6373348362743855,\n",
       "  0.6227591834962368,\n",
       "  0.6942213274538517,\n",
       "  0.7193172764033079,\n",
       "  2.550731545686722,\n",
       "  2.985149449110031,\n",
       "  0.5794439680874348,\n",
       "  0.6563787780702114,\n",
       "  0.6737937644124031,\n",
       "  0.5672128837555647,\n",
       "  0.870005989074707,\n",
       "  0.5272054748144,\n",
       "  0.554440176486969,\n",
       "  0.6500457551330328,\n",
       "  3.771521282196045,\n",
       "  2.650035160779953,\n",
       "  0.5729787111282348,\n",
       "  0.9393204525113106,\n",
       "  3.608709394931793,\n",
       "  0.9008684694766999,\n",
       "  0.7069461036473512,\n",
       "  0.5802313216496259,\n",
       "  0.6450423441827298,\n",
       "  1.244331419467926,\n",
       "  0.553926817048341,\n",
       "  0.5498048983514309,\n",
       "  0.5492208682000637,\n",
       "  0.5832105251960457,\n",
       "  0.5603693928569555,\n",
       "  0.5337116800248622,\n",
       "  0.6889535034541041,\n",
       "  0.6011752238497138,\n",
       "  0.6862565964460373,\n",
       "  0.5531947296112776,\n",
       "  0.5427584435790778,\n",
       "  0.5503311462700367,\n",
       "  0.5517601509130146,\n",
       "  0.5367279082536698,\n",
       "  2.54752054810524,\n",
       "  4.906094002723694,\n",
       "  9.92885103225708,\n",
       "  3.4750600337982176,\n",
       "  0.7413829120807349,\n",
       "  2.7921658158302307,\n",
       "  0.6531695540994406,\n",
       "  0.5759968988597393,\n",
       "  0.8828467488288879,\n",
       "  0.5456912063062191,\n",
       "  0.9159355618059635,\n",
       "  0.522395984083414,\n",
       "  0.9075418528635055,\n",
       "  4.442626953125,\n",
       "  7.767560821771622,\n",
       "  0.5967112246900796,\n",
       "  0.5542926535010337,\n",
       "  0.7225287243723869,\n",
       "  0.8979916259646415,\n",
       "  0.5630660690367222,\n",
       "  0.7376041576266289,\n",
       "  0.5829350061714649,\n",
       "  1.6777095317840576,\n",
       "  0.511769592948258,\n",
       "  0.564829969778657,\n",
       "  0.569826252758503,\n",
       "  0.5604617065284401,\n",
       "  0.591907930932939,\n",
       "  0.5564604382961988,\n",
       "  0.5496116623282432,\n",
       "  0.6579113990068436,\n",
       "  4.467308592796326,\n",
       "  2.090949738025665,\n",
       "  3.3116106897592545,\n",
       "  0.7542257843539119,\n",
       "  0.5523518776580204,\n",
       "  0.6658123522996903,\n",
       "  0.7049356706440448,\n",
       "  0.5282835721969604,\n",
       "  0.5603218057192862,\n",
       "  1.4116967871785164,\n",
       "  1.2964799858629703,\n",
       "  0.5454071320593357,\n",
       "  3.9601753652095795,\n",
       "  7.390788006782532,\n",
       "  2.408160275220871,\n",
       "  1.431661108136177,\n",
       "  2.004127186536789,\n",
       "  3.0094690918922424,\n",
       "  13.75387077331543,\n",
       "  6.068401467800141,\n",
       "  0.5322012424934656,\n",
       "  0.690264733787626,\n",
       "  0.5385294053703547,\n",
       "  0.7431977070868016,\n",
       "  0.5808646656572819,\n",
       "  0.6256781980395317,\n",
       "  0.5519132561981678,\n",
       "  0.6272472651675344,\n",
       "  0.5197511862963438,\n",
       "  0.5266077972948551,\n",
       "  0.554773635417223,\n",
       "  0.520250310190022,\n",
       "  0.7371049106121064,\n",
       "  0.8806909292936325,\n",
       "  2.501278005389031,\n",
       "  1.3685579359531403,\n",
       "  0.8410668969154358,\n",
       "  0.6059177195653319,\n",
       "  0.5927757825702429,\n",
       "  0.512636942602694,\n",
       "  0.589342413470149,\n",
       "  0.66898078545928,\n",
       "  0.7291197683662176,\n",
       "  0.6591323539614677,\n",
       "  0.5329935912042856,\n",
       "  1.2613463554531337,\n",
       "  1.043851485848427,\n",
       "  1.4161022753454744,\n",
       "  0.5204496828839182,\n",
       "  0.5305062349885702,\n",
       "  0.6793022289872169,\n",
       "  0.5107326129684224,\n",
       "  0.5137807443737984,\n",
       "  0.787365884706378,\n",
       "  0.49302118085324764,\n",
       "  0.6372616270091385,\n",
       "  4.250388273596764,\n",
       "  2.2035419762134554,\n",
       "  2.336372983455658,\n",
       "  1.6412193089723588,\n",
       "  0.5731876626610756,\n",
       "  0.7149516008794308,\n",
       "  0.7398112624883652,\n",
       "  0.7252553605940193,\n",
       "  0.5034627355635166,\n",
       "  0.5415327908471227,\n",
       "  0.5078989751636982,\n",
       "  0.8379516210407019,\n",
       "  5.020260202884674,\n",
       "  1.9189450562000274,\n",
       "  1.9045753836631776,\n",
       "  0.5105962681409437,\n",
       "  0.5283342435956001,\n",
       "  0.7942133557051421,\n",
       "  0.7708092212677002,\n",
       "  0.5945615917444229,\n",
       "  0.5782835766673088,\n",
       "  0.49511607238091526,\n",
       "  0.5441994376480579,\n",
       "  0.5856028160080313,\n",
       "  0.7321796596050263,\n",
       "  0.6144633609801531,\n",
       "  0.6015920888632535,\n",
       "  0.6265232473611831,\n",
       "  1.8633198738098145,\n",
       "  2.2499433398246764,\n",
       "  0.5274443790316582,\n",
       "  0.4890538577221378,\n",
       "  0.549083049967885,\n",
       "  0.7374102577567101,\n",
       "  0.7331961406394839,\n",
       "  0.6577908150851727,\n",
       "  0.5119014862924814,\n",
       "  0.5633708868874236,\n",
       "  0.5538820177316666,\n",
       "  1.045443357527256,\n",
       "  0.9338945746421814,\n",
       "  0.9194879680871964,\n",
       "  0.5765351135283708,\n",
       "  0.5383061174303293])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(100)\n",
    "train_model(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68aaa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int64\n",
      "Accuracy: 0.9587500095367432\n"
     ]
    }
   ],
   "source": [
    "# do not remove â€“ nothing to code here\n",
    "# run this cell before moving on\n",
    "# ensure get_accuracy from task 1.5 is defined\n",
    "\n",
    "def get_accuracy(pred: torch.Tensor, label: torch.Tensor):\n",
    "    y_pred = (pred > 0.5).long()\n",
    "    label = label.long()\n",
    "\n",
    "    return (y_pred == label).float().mean()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        pred = model(x)\n",
    "        y = y.view(-1, 1).long()\n",
    "        print(pred.dtype, y.dtype)\n",
    "        acc = get_accuracy(pred, y)\n",
    "        print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c71e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2109s-ay2526s1-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
