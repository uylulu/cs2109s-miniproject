{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdcb2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77dc61",
   "metadata": {},
   "source": [
    "### Load the dataset and pre-process them into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21214c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"data/cipher_objective.csv\")\n",
    "df_x = data_set[\"text\"]\n",
    "df_y = data_set[\"class\"]\n",
    "\n",
    "arr_x = []\n",
    "arr_y = []\n",
    "\n",
    "for i in range(len(df_x)):\n",
    "    text = df_x.iloc[i]\n",
    "    temp = [ord(c) for c in text]\n",
    "    arr_x.append(temp)\n",
    "\n",
    "    label = df_y.iloc[i]\n",
    "    arr_y.append(0 if label == \"exit\" else 1)\n",
    "\n",
    "data_x = torch.tensor(arr_x, dtype=torch.float32)\n",
    "data_y = torch.tensor(arr_y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2d8fe",
   "metadata": {},
   "source": [
    "### Split the dataset and load them into data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51c1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3200, 100]) torch.Size([3200]) torch.Size([800, 100]) torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  data_x, data_y,\n",
    "  test_size=0.2,\n",
    "  stratify=data_y,\n",
    "  random_state=42\n",
    ")\n",
    "\n",
    "train_x = X_train.detach().clone()\n",
    "train_y = y_train.detach().clone()\n",
    "test_x = X_test.detach().clone()\n",
    "test_y = y_test.detach().clone()\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0c8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1600, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a460850",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb6f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, input_size: int):\n",
    "    super().__init__()\n",
    "    self.l1 = nn.Linear(input_size, input_size, True)\n",
    "    self.l2 = nn.Linear(input_size, 1, True)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.l1(x)\n",
    "    x = self.l2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2d7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(loader: torch.utils.data.DataLoader, model: nn.Module):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model.to(device)\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  \n",
    "\n",
    "    epoch_losses = []\n",
    "    for i in range(300):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for _, (x, y) in enumerate(loader):\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            y = y.view(-1, 1).long()   \n",
    "            loss = loss_fn(y_pred, y.float())\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(loader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "        \n",
    "\n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d025e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 24.509523300024178\n",
      "Epoch: 1, Loss: 2.611325447375958\n",
      "Epoch: 2, Loss: 0.4755720289853903\n",
      "Epoch: 3, Loss: 0.43251289083407474\n",
      "Epoch: 4, Loss: 0.4521123766899109\n",
      "Epoch: 5, Loss: 0.37471545201081496\n",
      "Epoch: 6, Loss: 0.44176454498217654\n",
      "Epoch: 7, Loss: 0.38695438320820147\n",
      "Epoch: 8, Loss: 0.39326858062010545\n",
      "Epoch: 9, Loss: 0.39264396062264073\n",
      "Epoch: 10, Loss: 0.3937067481187674\n",
      "Epoch: 11, Loss: 0.3913080463042626\n",
      "Epoch: 12, Loss: 0.31762741849972653\n",
      "Epoch: 13, Loss: 0.2972530470444606\n",
      "Epoch: 14, Loss: 0.4351063599953285\n",
      "Epoch: 15, Loss: 0.2809518781992105\n",
      "Epoch: 16, Loss: 0.4787548780441284\n",
      "Epoch: 17, Loss: 0.3974417791916774\n",
      "Epoch: 18, Loss: 0.28825093232668364\n",
      "Epoch: 19, Loss: 0.2703788624360011\n",
      "Epoch: 20, Loss: 0.39315657432262713\n",
      "Epoch: 21, Loss: 0.2666893142920274\n",
      "Epoch: 22, Loss: 0.23404309382805458\n",
      "Epoch: 23, Loss: 0.5518578439950943\n",
      "Epoch: 24, Loss: 0.30162755113381606\n",
      "Epoch: 25, Loss: 0.30102454125881195\n",
      "Epoch: 26, Loss: 0.27570166610754454\n",
      "Epoch: 27, Loss: 0.39010913784687334\n",
      "Epoch: 28, Loss: 0.24820623833399552\n",
      "Epoch: 29, Loss: 0.2533174088368049\n",
      "Epoch: 30, Loss: 0.45712697162077975\n",
      "Epoch: 31, Loss: 0.35251934024003834\n",
      "Epoch: 32, Loss: 0.23798309495815864\n",
      "Epoch: 33, Loss: 0.254391896036955\n",
      "Epoch: 34, Loss: 0.3128305799685992\n",
      "Epoch: 35, Loss: 0.37290606246544766\n",
      "Epoch: 36, Loss: 0.3215234245245273\n",
      "Epoch: 37, Loss: 0.31187910070786107\n",
      "Epoch: 38, Loss: 0.222883661205952\n",
      "Epoch: 39, Loss: 0.279375366293467\n",
      "Epoch: 40, Loss: 0.46180509145443255\n",
      "Epoch: 41, Loss: 0.2654048032485522\n",
      "Epoch: 42, Loss: 0.22884403856901023\n",
      "Epoch: 43, Loss: 0.3278026282787323\n",
      "Epoch: 44, Loss: 0.2743411832130872\n",
      "Epoch: 45, Loss: 0.27673208942780125\n",
      "Epoch: 46, Loss: 0.19675343655622923\n",
      "Epoch: 47, Loss: 0.2113854610002958\n",
      "Epoch: 48, Loss: 0.217967784175506\n",
      "Epoch: 49, Loss: 0.4357434625809009\n",
      "Epoch: 50, Loss: 0.2198236630513118\n",
      "Epoch: 51, Loss: 0.3084386701767261\n",
      "Epoch: 52, Loss: 0.27891344749010527\n",
      "Epoch: 53, Loss: 0.28353060896580035\n",
      "Epoch: 54, Loss: 0.38630146246690017\n",
      "Epoch: 55, Loss: 0.19617356818455917\n",
      "Epoch: 56, Loss: 0.230173164835343\n",
      "Epoch: 57, Loss: 0.3153636180437528\n",
      "Epoch: 58, Loss: 0.2574036820576741\n",
      "Epoch: 59, Loss: 0.2545711317887673\n",
      "Epoch: 60, Loss: 0.18951672315597534\n",
      "Epoch: 61, Loss: 0.33371534599707675\n",
      "Epoch: 62, Loss: 0.19453252508090094\n",
      "Epoch: 63, Loss: 0.18440335072003877\n",
      "Epoch: 64, Loss: 0.31517262069078594\n",
      "Epoch: 65, Loss: 0.346862786091291\n",
      "Epoch: 66, Loss: 0.21295164410884565\n",
      "Epoch: 67, Loss: 0.1963667216209265\n",
      "Epoch: 68, Loss: 0.19477047484654647\n",
      "Epoch: 69, Loss: 0.4443416790320323\n",
      "Epoch: 70, Loss: 0.18674791546968314\n",
      "Epoch: 71, Loss: 0.23399260067022765\n",
      "Epoch: 72, Loss: 0.16312499115100274\n",
      "Epoch: 73, Loss: 0.3897943382079785\n",
      "Epoch: 74, Loss: 0.2449072152376175\n",
      "Epoch: 75, Loss: 0.2217891147503486\n",
      "Epoch: 76, Loss: 0.16957653027314407\n",
      "Epoch: 77, Loss: 0.16249347993960747\n",
      "Epoch: 78, Loss: 0.18103588200532472\n",
      "Epoch: 79, Loss: 0.32297079265117645\n",
      "Epoch: 80, Loss: 0.35390953719615936\n",
      "Epoch: 81, Loss: 0.1707951701604403\n",
      "Epoch: 82, Loss: 0.15558673040224955\n",
      "Epoch: 83, Loss: 0.20684327299778277\n",
      "Epoch: 84, Loss: 0.3743466276388902\n",
      "Epoch: 85, Loss: 0.2695451516371507\n",
      "Epoch: 86, Loss: 0.18285303161694452\n",
      "Epoch: 87, Loss: 0.20186594587105972\n",
      "Epoch: 88, Loss: 0.2128984286234929\n",
      "Epoch: 89, Loss: 0.18888241511124831\n",
      "Epoch: 90, Loss: 0.21869671803254348\n",
      "Epoch: 91, Loss: 0.3601524829864502\n",
      "Epoch: 92, Loss: 0.2599879124989876\n",
      "Epoch: 93, Loss: 0.19040170655800745\n",
      "Epoch: 94, Loss: 0.17505273280235437\n",
      "Epoch: 95, Loss: 0.19785364201435676\n",
      "Epoch: 96, Loss: 0.24430051445960999\n",
      "Epoch: 97, Loss: 0.21526351800331703\n",
      "Epoch: 98, Loss: 0.16092330675858718\n",
      "Epoch: 99, Loss: 0.18768896735631502\n",
      "Epoch: 100, Loss: 0.20696666951362902\n",
      "Epoch: 101, Loss: 0.25572832845724547\n",
      "Epoch: 102, Loss: 0.17356146757419294\n",
      "Epoch: 103, Loss: 0.1542669110573255\n",
      "Epoch: 104, Loss: 0.18608757165762094\n",
      "Epoch: 105, Loss: 0.38830506113859325\n",
      "Epoch: 106, Loss: 0.1691981439407055\n",
      "Epoch: 107, Loss: 0.14167694117014223\n",
      "Epoch: 108, Loss: 0.17774435935112146\n",
      "Epoch: 109, Loss: 0.1934071068580334\n",
      "Epoch: 110, Loss: 0.19683405183828795\n",
      "Epoch: 111, Loss: 0.3620141458052855\n",
      "Epoch: 112, Loss: 0.15432094381405756\n",
      "Epoch: 113, Loss: 0.22854204246631035\n",
      "Epoch: 114, Loss: 0.16214840171428826\n",
      "Epoch: 115, Loss: 0.1636036347884398\n",
      "Epoch: 116, Loss: 0.24803033127234533\n",
      "Epoch: 117, Loss: 0.27120851897276366\n",
      "Epoch: 118, Loss: 0.18412743852688715\n",
      "Epoch: 119, Loss: 0.22993109547174895\n",
      "Epoch: 120, Loss: 0.13851895068700498\n",
      "Epoch: 121, Loss: 0.15507297619031027\n",
      "Epoch: 122, Loss: 0.1504779618520003\n",
      "Epoch: 123, Loss: 0.2049847090473542\n",
      "Epoch: 124, Loss: 0.15306179110820478\n",
      "Epoch: 125, Loss: 0.15014254588347214\n",
      "Epoch: 126, Loss: 0.14837088894385558\n",
      "Epoch: 127, Loss: 0.19737912599857038\n",
      "Epoch: 128, Loss: 0.4433313768643599\n",
      "Epoch: 129, Loss: 0.17421240302232596\n",
      "Epoch: 130, Loss: 0.2562442593849622\n",
      "Epoch: 131, Loss: 0.15038307011127472\n",
      "Epoch: 132, Loss: 0.1479634132522803\n",
      "Epoch: 133, Loss: 0.20382758344595248\n",
      "Epoch: 134, Loss: 0.14287514354173952\n",
      "Epoch: 135, Loss: 0.13517598693187421\n",
      "Epoch: 136, Loss: 0.25884681596205783\n",
      "Epoch: 137, Loss: 0.2846762067996539\n",
      "Epoch: 138, Loss: 0.1732053762445083\n",
      "Epoch: 139, Loss: 0.26939182441968185\n",
      "Epoch: 140, Loss: 0.19109583015625292\n",
      "Epoch: 141, Loss: 0.16033789630119616\n",
      "Epoch: 142, Loss: 0.1549502255824896\n",
      "Epoch: 143, Loss: 0.17728523222299722\n",
      "Epoch: 144, Loss: 0.2817493126942561\n",
      "Epoch: 145, Loss: 0.15093151537271646\n",
      "Epoch: 146, Loss: 0.13888975576712534\n",
      "Epoch: 147, Loss: 0.1434584047931891\n",
      "Epoch: 148, Loss: 0.1511605135523356\n",
      "Epoch: 149, Loss: 0.17339747456403878\n",
      "Epoch: 150, Loss: 0.14490028757315415\n",
      "Epoch: 151, Loss: 0.14910239783617166\n",
      "Epoch: 152, Loss: 0.14058487461163446\n",
      "Epoch: 153, Loss: 0.18532959658365983\n",
      "Epoch: 154, Loss: 0.31436986132309985\n",
      "Epoch: 155, Loss: 0.40423120672886187\n",
      "Epoch: 156, Loss: 0.13498283521487162\n",
      "Epoch: 157, Loss: 0.15404037156930336\n",
      "Epoch: 158, Loss: 0.16798109503892752\n",
      "Epoch: 159, Loss: 0.170134764451247\n",
      "Epoch: 160, Loss: 0.16630912342896828\n",
      "Epoch: 161, Loss: 0.22094779862807348\n",
      "Epoch: 162, Loss: 0.13886160518114382\n",
      "Epoch: 163, Loss: 0.26360602332995486\n",
      "Epoch: 164, Loss: 0.23532223644164893\n",
      "Epoch: 165, Loss: 0.16118707049351472\n",
      "Epoch: 166, Loss: 0.17735731601715088\n",
      "Epoch: 167, Loss: 0.1429677250293585\n",
      "Epoch: 168, Loss: 0.13232437521219254\n",
      "Epoch: 169, Loss: 0.13582109545285886\n",
      "Epoch: 170, Loss: 0.13307690161925095\n",
      "Epoch: 171, Loss: 0.16586208228881544\n",
      "Epoch: 172, Loss: 0.1272007255600049\n",
      "Epoch: 173, Loss: 0.1216746259194154\n",
      "Epoch: 174, Loss: 0.12366976588964462\n",
      "Epoch: 175, Loss: 0.17486207817609495\n",
      "Epoch: 176, Loss: 0.3132048272169553\n",
      "Epoch: 177, Loss: 0.1579262104171973\n",
      "Epoch: 178, Loss: 0.12370552007968609\n",
      "Epoch: 179, Loss: 0.12718635625564134\n",
      "Epoch: 180, Loss: 0.12896964355156973\n",
      "Epoch: 181, Loss: 0.22224048926280096\n",
      "Epoch: 182, Loss: 0.3418732445973616\n",
      "Epoch: 183, Loss: 0.12437439308716701\n",
      "Epoch: 184, Loss: 0.14075666608718726\n",
      "Epoch: 185, Loss: 0.2719526245043828\n",
      "Epoch: 186, Loss: 0.129542401776864\n",
      "Epoch: 187, Loss: 0.1351303607225418\n",
      "Epoch: 188, Loss: 0.2762746862494029\n",
      "Epoch: 189, Loss: 0.130894008737344\n",
      "Epoch: 190, Loss: 0.1556156827853276\n",
      "Epoch: 191, Loss: 0.13024129202732673\n",
      "Epoch: 192, Loss: 0.13501657430942243\n",
      "Epoch: 193, Loss: 0.2320682366306965\n",
      "Epoch: 194, Loss: 0.15380898404579896\n",
      "Epoch: 195, Loss: 0.12936367094516754\n",
      "Epoch: 196, Loss: 0.12482073673835167\n",
      "Epoch: 197, Loss: 0.1902277865088903\n",
      "Epoch: 198, Loss: 0.2393401603286083\n",
      "Epoch: 199, Loss: 0.12129460447109662\n",
      "Epoch: 200, Loss: 0.23185005210913145\n",
      "Epoch: 201, Loss: 0.16215707247073835\n",
      "Epoch: 202, Loss: 0.18944182648108557\n",
      "Epoch: 203, Loss: 0.16535722349698728\n",
      "Epoch: 204, Loss: 0.13230364081951287\n",
      "Epoch: 205, Loss: 0.12536642356560782\n",
      "Epoch: 206, Loss: 0.1485553699044081\n",
      "Epoch: 207, Loss: 0.12530816117158303\n",
      "Epoch: 208, Loss: 0.18244226104938066\n",
      "Epoch: 209, Loss: 0.27282184018538547\n",
      "Epoch: 210, Loss: 0.20450484179533446\n",
      "Epoch: 211, Loss: 0.12663241418508384\n",
      "Epoch: 212, Loss: 0.1773089858201834\n",
      "Epoch: 213, Loss: 0.12221176062638943\n",
      "Epoch: 214, Loss: 0.1533552700510392\n",
      "Epoch: 215, Loss: 0.15228619598425353\n",
      "Epoch: 216, Loss: 0.13144581191814864\n",
      "Epoch: 217, Loss: 0.17267754616645667\n",
      "Epoch: 218, Loss: 0.1258506803558423\n",
      "Epoch: 219, Loss: 0.14677554540909254\n",
      "Epoch: 220, Loss: 0.12940068256396514\n",
      "Epoch: 221, Loss: 0.26814874089681184\n",
      "Epoch: 222, Loss: 0.158639411513622\n",
      "Epoch: 223, Loss: 0.17662153106469375\n",
      "Epoch: 224, Loss: 0.3202479218061154\n",
      "Epoch: 225, Loss: 0.12230188628801933\n",
      "Epoch: 226, Loss: 0.14983985343804726\n",
      "Epoch: 227, Loss: 0.12391307606146885\n",
      "Epoch: 228, Loss: 0.12291316229563493\n",
      "Epoch: 229, Loss: 0.11997058070622958\n",
      "Epoch: 230, Loss: 0.13500100947343385\n",
      "Epoch: 231, Loss: 0.12246085015627053\n",
      "Epoch: 232, Loss: 0.1265200938169773\n",
      "Epoch: 233, Loss: 0.11963861836836888\n",
      "Epoch: 234, Loss: 0.12407984126072663\n",
      "Epoch: 235, Loss: 0.21777149977592322\n",
      "Epoch: 236, Loss: 0.11998138691370304\n",
      "Epoch: 237, Loss: 0.11936034434116803\n",
      "Epoch: 238, Loss: 0.209467871257892\n",
      "Epoch: 239, Loss: 0.1649763939472345\n",
      "Epoch: 240, Loss: 0.13955529779195786\n",
      "Epoch: 241, Loss: 0.1268118774661651\n",
      "Epoch: 242, Loss: 0.1311863294014564\n",
      "Epoch: 243, Loss: 0.1436406379708877\n",
      "Epoch: 244, Loss: 0.24578508620078748\n",
      "Epoch: 245, Loss: 0.13618751271412924\n",
      "Epoch: 246, Loss: 0.15950043499469757\n",
      "Epoch: 247, Loss: 0.1491666636787928\n",
      "Epoch: 248, Loss: 0.1283744522012197\n",
      "Epoch: 249, Loss: 0.11537224971331082\n",
      "Epoch: 250, Loss: 0.12051446048113015\n",
      "Epoch: 251, Loss: 0.13288605098540968\n",
      "Epoch: 252, Loss: 0.21599284559488297\n",
      "Epoch: 253, Loss: 0.16217996351994002\n",
      "Epoch: 254, Loss: 0.16321254464296195\n",
      "Epoch: 255, Loss: 0.27839227135364825\n",
      "Epoch: 256, Loss: 0.14083149857245958\n",
      "Epoch: 257, Loss: 0.11966414061876443\n",
      "Epoch: 258, Loss: 0.12608066602395132\n",
      "Epoch: 259, Loss: 0.13479983520049316\n",
      "Epoch: 260, Loss: 0.1277507830124635\n",
      "Epoch: 261, Loss: 0.1927578655573038\n",
      "Epoch: 262, Loss: 0.2972742250332466\n",
      "Epoch: 263, Loss: 0.12432325688692239\n",
      "Epoch: 264, Loss: 0.13514981074975088\n",
      "Epoch: 265, Loss: 0.11514080373140481\n",
      "Epoch: 266, Loss: 0.19948352128267288\n",
      "Epoch: 267, Loss: 0.18106875797876945\n",
      "Epoch: 268, Loss: 0.3253312294299786\n",
      "Epoch: 269, Loss: 0.12186962824601394\n",
      "Epoch: 270, Loss: 0.12099485271252118\n",
      "Epoch: 271, Loss: 0.14434728427575186\n",
      "Epoch: 272, Loss: 0.14948901075583237\n",
      "Epoch: 273, Loss: 0.12722223080121553\n",
      "Epoch: 274, Loss: 0.11726187398800483\n",
      "Epoch: 275, Loss: 0.11660731583833694\n",
      "Epoch: 276, Loss: 0.1227723084963285\n",
      "Epoch: 277, Loss: 0.120722828576198\n",
      "Epoch: 278, Loss: 0.11530899657652928\n",
      "Epoch: 279, Loss: 0.13168727205349848\n",
      "Epoch: 280, Loss: 0.15173112887602586\n",
      "Epoch: 281, Loss: 0.2371925447995846\n",
      "Epoch: 282, Loss: 0.13376181973860815\n",
      "Epoch: 283, Loss: 0.12703363654705194\n",
      "Epoch: 284, Loss: 0.13446243737752622\n",
      "Epoch: 285, Loss: 0.1339255187373895\n",
      "Epoch: 286, Loss: 0.13786786450789526\n",
      "Epoch: 287, Loss: 0.12426309975293967\n",
      "Epoch: 288, Loss: 0.1487053970877941\n",
      "Epoch: 289, Loss: 0.12135342451242301\n",
      "Epoch: 290, Loss: 0.1305133757682947\n",
      "Epoch: 291, Loss: 0.11971942106118569\n",
      "Epoch: 292, Loss: 0.13641903778681388\n",
      "Epoch: 293, Loss: 0.1719818854561219\n",
      "Epoch: 294, Loss: 0.12888295547320291\n",
      "Epoch: 295, Loss: 0.1467976633172769\n",
      "Epoch: 296, Loss: 0.17811726664121336\n",
      "Epoch: 297, Loss: 0.19468915749054688\n",
      "Epoch: 298, Loss: 0.17418673462592638\n",
      "Epoch: 299, Loss: 0.1762335283251909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Model(\n",
       "   (l1): Linear(in_features=100, out_features=100, bias=True)\n",
       "   (l2): Linear(in_features=100, out_features=1, bias=True)\n",
       " ),\n",
       " [24.509523300024178,\n",
       "  2.611325447375958,\n",
       "  0.4755720289853903,\n",
       "  0.43251289083407474,\n",
       "  0.4521123766899109,\n",
       "  0.37471545201081496,\n",
       "  0.44176454498217654,\n",
       "  0.38695438320820147,\n",
       "  0.39326858062010545,\n",
       "  0.39264396062264073,\n",
       "  0.3937067481187674,\n",
       "  0.3913080463042626,\n",
       "  0.31762741849972653,\n",
       "  0.2972530470444606,\n",
       "  0.4351063599953285,\n",
       "  0.2809518781992105,\n",
       "  0.4787548780441284,\n",
       "  0.3974417791916774,\n",
       "  0.28825093232668364,\n",
       "  0.2703788624360011,\n",
       "  0.39315657432262713,\n",
       "  0.2666893142920274,\n",
       "  0.23404309382805458,\n",
       "  0.5518578439950943,\n",
       "  0.30162755113381606,\n",
       "  0.30102454125881195,\n",
       "  0.27570166610754454,\n",
       "  0.39010913784687334,\n",
       "  0.24820623833399552,\n",
       "  0.2533174088368049,\n",
       "  0.45712697162077975,\n",
       "  0.35251934024003834,\n",
       "  0.23798309495815864,\n",
       "  0.254391896036955,\n",
       "  0.3128305799685992,\n",
       "  0.37290606246544766,\n",
       "  0.3215234245245273,\n",
       "  0.31187910070786107,\n",
       "  0.222883661205952,\n",
       "  0.279375366293467,\n",
       "  0.46180509145443255,\n",
       "  0.2654048032485522,\n",
       "  0.22884403856901023,\n",
       "  0.3278026282787323,\n",
       "  0.2743411832130872,\n",
       "  0.27673208942780125,\n",
       "  0.19675343655622923,\n",
       "  0.2113854610002958,\n",
       "  0.217967784175506,\n",
       "  0.4357434625809009,\n",
       "  0.2198236630513118,\n",
       "  0.3084386701767261,\n",
       "  0.27891344749010527,\n",
       "  0.28353060896580035,\n",
       "  0.38630146246690017,\n",
       "  0.19617356818455917,\n",
       "  0.230173164835343,\n",
       "  0.3153636180437528,\n",
       "  0.2574036820576741,\n",
       "  0.2545711317887673,\n",
       "  0.18951672315597534,\n",
       "  0.33371534599707675,\n",
       "  0.19453252508090094,\n",
       "  0.18440335072003877,\n",
       "  0.31517262069078594,\n",
       "  0.346862786091291,\n",
       "  0.21295164410884565,\n",
       "  0.1963667216209265,\n",
       "  0.19477047484654647,\n",
       "  0.4443416790320323,\n",
       "  0.18674791546968314,\n",
       "  0.23399260067022765,\n",
       "  0.16312499115100274,\n",
       "  0.3897943382079785,\n",
       "  0.2449072152376175,\n",
       "  0.2217891147503486,\n",
       "  0.16957653027314407,\n",
       "  0.16249347993960747,\n",
       "  0.18103588200532472,\n",
       "  0.32297079265117645,\n",
       "  0.35390953719615936,\n",
       "  0.1707951701604403,\n",
       "  0.15558673040224955,\n",
       "  0.20684327299778277,\n",
       "  0.3743466276388902,\n",
       "  0.2695451516371507,\n",
       "  0.18285303161694452,\n",
       "  0.20186594587105972,\n",
       "  0.2128984286234929,\n",
       "  0.18888241511124831,\n",
       "  0.21869671803254348,\n",
       "  0.3601524829864502,\n",
       "  0.2599879124989876,\n",
       "  0.19040170655800745,\n",
       "  0.17505273280235437,\n",
       "  0.19785364201435676,\n",
       "  0.24430051445960999,\n",
       "  0.21526351800331703,\n",
       "  0.16092330675858718,\n",
       "  0.18768896735631502,\n",
       "  0.20696666951362902,\n",
       "  0.25572832845724547,\n",
       "  0.17356146757419294,\n",
       "  0.1542669110573255,\n",
       "  0.18608757165762094,\n",
       "  0.38830506113859325,\n",
       "  0.1691981439407055,\n",
       "  0.14167694117014223,\n",
       "  0.17774435935112146,\n",
       "  0.1934071068580334,\n",
       "  0.19683405183828795,\n",
       "  0.3620141458052855,\n",
       "  0.15432094381405756,\n",
       "  0.22854204246631035,\n",
       "  0.16214840171428826,\n",
       "  0.1636036347884398,\n",
       "  0.24803033127234533,\n",
       "  0.27120851897276366,\n",
       "  0.18412743852688715,\n",
       "  0.22993109547174895,\n",
       "  0.13851895068700498,\n",
       "  0.15507297619031027,\n",
       "  0.1504779618520003,\n",
       "  0.2049847090473542,\n",
       "  0.15306179110820478,\n",
       "  0.15014254588347214,\n",
       "  0.14837088894385558,\n",
       "  0.19737912599857038,\n",
       "  0.4433313768643599,\n",
       "  0.17421240302232596,\n",
       "  0.2562442593849622,\n",
       "  0.15038307011127472,\n",
       "  0.1479634132522803,\n",
       "  0.20382758344595248,\n",
       "  0.14287514354173952,\n",
       "  0.13517598693187421,\n",
       "  0.25884681596205783,\n",
       "  0.2846762067996539,\n",
       "  0.1732053762445083,\n",
       "  0.26939182441968185,\n",
       "  0.19109583015625292,\n",
       "  0.16033789630119616,\n",
       "  0.1549502255824896,\n",
       "  0.17728523222299722,\n",
       "  0.2817493126942561,\n",
       "  0.15093151537271646,\n",
       "  0.13888975576712534,\n",
       "  0.1434584047931891,\n",
       "  0.1511605135523356,\n",
       "  0.17339747456403878,\n",
       "  0.14490028757315415,\n",
       "  0.14910239783617166,\n",
       "  0.14058487461163446,\n",
       "  0.18532959658365983,\n",
       "  0.31436986132309985,\n",
       "  0.40423120672886187,\n",
       "  0.13498283521487162,\n",
       "  0.15404037156930336,\n",
       "  0.16798109503892752,\n",
       "  0.170134764451247,\n",
       "  0.16630912342896828,\n",
       "  0.22094779862807348,\n",
       "  0.13886160518114382,\n",
       "  0.26360602332995486,\n",
       "  0.23532223644164893,\n",
       "  0.16118707049351472,\n",
       "  0.17735731601715088,\n",
       "  0.1429677250293585,\n",
       "  0.13232437521219254,\n",
       "  0.13582109545285886,\n",
       "  0.13307690161925095,\n",
       "  0.16586208228881544,\n",
       "  0.1272007255600049,\n",
       "  0.1216746259194154,\n",
       "  0.12366976588964462,\n",
       "  0.17486207817609495,\n",
       "  0.3132048272169553,\n",
       "  0.1579262104171973,\n",
       "  0.12370552007968609,\n",
       "  0.12718635625564134,\n",
       "  0.12896964355156973,\n",
       "  0.22224048926280096,\n",
       "  0.3418732445973616,\n",
       "  0.12437439308716701,\n",
       "  0.14075666608718726,\n",
       "  0.2719526245043828,\n",
       "  0.129542401776864,\n",
       "  0.1351303607225418,\n",
       "  0.2762746862494029,\n",
       "  0.130894008737344,\n",
       "  0.1556156827853276,\n",
       "  0.13024129202732673,\n",
       "  0.13501657430942243,\n",
       "  0.2320682366306965,\n",
       "  0.15380898404579896,\n",
       "  0.12936367094516754,\n",
       "  0.12482073673835167,\n",
       "  0.1902277865088903,\n",
       "  0.2393401603286083,\n",
       "  0.12129460447109662,\n",
       "  0.23185005210913145,\n",
       "  0.16215707247073835,\n",
       "  0.18944182648108557,\n",
       "  0.16535722349698728,\n",
       "  0.13230364081951287,\n",
       "  0.12536642356560782,\n",
       "  0.1485553699044081,\n",
       "  0.12530816117158303,\n",
       "  0.18244226104938066,\n",
       "  0.27282184018538547,\n",
       "  0.20450484179533446,\n",
       "  0.12663241418508384,\n",
       "  0.1773089858201834,\n",
       "  0.12221176062638943,\n",
       "  0.1533552700510392,\n",
       "  0.15228619598425353,\n",
       "  0.13144581191814864,\n",
       "  0.17267754616645667,\n",
       "  0.1258506803558423,\n",
       "  0.14677554540909254,\n",
       "  0.12940068256396514,\n",
       "  0.26814874089681184,\n",
       "  0.158639411513622,\n",
       "  0.17662153106469375,\n",
       "  0.3202479218061154,\n",
       "  0.12230188628801933,\n",
       "  0.14983985343804726,\n",
       "  0.12391307606146885,\n",
       "  0.12291316229563493,\n",
       "  0.11997058070622958,\n",
       "  0.13500100947343385,\n",
       "  0.12246085015627053,\n",
       "  0.1265200938169773,\n",
       "  0.11963861836836888,\n",
       "  0.12407984126072663,\n",
       "  0.21777149977592322,\n",
       "  0.11998138691370304,\n",
       "  0.11936034434116803,\n",
       "  0.209467871257892,\n",
       "  0.1649763939472345,\n",
       "  0.13955529779195786,\n",
       "  0.1268118774661651,\n",
       "  0.1311863294014564,\n",
       "  0.1436406379708877,\n",
       "  0.24578508620078748,\n",
       "  0.13618751271412924,\n",
       "  0.15950043499469757,\n",
       "  0.1491666636787928,\n",
       "  0.1283744522012197,\n",
       "  0.11537224971331082,\n",
       "  0.12051446048113015,\n",
       "  0.13288605098540968,\n",
       "  0.21599284559488297,\n",
       "  0.16217996351994002,\n",
       "  0.16321254464296195,\n",
       "  0.27839227135364825,\n",
       "  0.14083149857245958,\n",
       "  0.11966414061876443,\n",
       "  0.12608066602395132,\n",
       "  0.13479983520049316,\n",
       "  0.1277507830124635,\n",
       "  0.1927578655573038,\n",
       "  0.2972742250332466,\n",
       "  0.12432325688692239,\n",
       "  0.13514981074975088,\n",
       "  0.11514080373140481,\n",
       "  0.19948352128267288,\n",
       "  0.18106875797876945,\n",
       "  0.3253312294299786,\n",
       "  0.12186962824601394,\n",
       "  0.12099485271252118,\n",
       "  0.14434728427575186,\n",
       "  0.14948901075583237,\n",
       "  0.12722223080121553,\n",
       "  0.11726187398800483,\n",
       "  0.11660731583833694,\n",
       "  0.1227723084963285,\n",
       "  0.120722828576198,\n",
       "  0.11530899657652928,\n",
       "  0.13168727205349848,\n",
       "  0.15173112887602586,\n",
       "  0.2371925447995846,\n",
       "  0.13376181973860815,\n",
       "  0.12703363654705194,\n",
       "  0.13446243737752622,\n",
       "  0.1339255187373895,\n",
       "  0.13786786450789526,\n",
       "  0.12426309975293967,\n",
       "  0.1487053970877941,\n",
       "  0.12135342451242301,\n",
       "  0.1305133757682947,\n",
       "  0.11971942106118569,\n",
       "  0.13641903778681388,\n",
       "  0.1719818854561219,\n",
       "  0.12888295547320291,\n",
       "  0.1467976633172769,\n",
       "  0.17811726664121336,\n",
       "  0.19468915749054688,\n",
       "  0.17418673462592638,\n",
       "  0.1762335283251909])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(100)\n",
    "train_model(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e68aaa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int64\n",
      "Accuracy: 0.9587500095367432\n"
     ]
    }
   ],
   "source": [
    "# do not remove â€“ nothing to code here\n",
    "# run this cell before moving on\n",
    "# ensure get_accuracy from task 1.5 is defined\n",
    "\n",
    "def get_accuracy(pred: torch.Tensor, label: torch.Tensor):\n",
    "    y_pred = (pred > 0.5).long()\n",
    "    label = label.long()\n",
    "\n",
    "    return (y_pred == label).float().mean()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, y = data\n",
    "        pred = model(x)\n",
    "        y = y.view(-1, 1).long()\n",
    "        print(pred.dtype, y.dtype)\n",
    "        acc = get_accuracy(pred, y)\n",
    "        print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729c71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ciphertext-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952785d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs2109s-ay2526s1-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
